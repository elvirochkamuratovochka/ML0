## ML0
Метрические алгоритмы классификации
Метрические методы обучения методы, основанные на анализе сходтства объектов. Метрические алгоритмы классификации с обучающей выборкой  относят u к тому классу , для которого суммарный вес ближайших обучающих объектов  наибольший.
Метод 1: Метод ближайшего соседа (1nn)
Алгоритм ближайшего соседа (nearest neighbor, NN) является самым простым алгоритмом классификации. Он относит классифицируемый объект u к тому классу, которому принадлежит ближайший обучающий объект:

где  - это алгоритм,  - обучающая выборка,  - классифицируемый объект,  - класс, которому алгоритм дает предпочтение при классификации объекта u
Обучение NN сводится к запоминанию выборки.
вход :
: matrix
 обучающая выборка, на последнем месте метка класса;
: vector
 классифицируемый объект;
: number
 определить функцию расстояния;
выход:
имя класса
Сам алгоритм:
1.находим расстояние от точки u до точек из выборки, образуя новый вектор
2.находим минимальное расстояние в векторе и запоминаем точку А(это именно та точка расстояние до которой минимально)
3.узнаем какому классу принадлежит точка А и относим u к тому классу какому принадлежит точка А
На языке R алгоритм реализован следующим образом : 1NN.R
1NN and Shiny
Метод k-ближайших соседей (knn)
Чтобы сгладить влияние шумовых выбросов, будем классифицировать объекты путём голосования по k ближайшим соседям.

вход :
: matrix
 обучающая выборка, на последнем месте метка класса
: vector
 классифицируемый объект
: number
 определить функцию расстояния
: number
кол-во соседей  
выход
имя класса
Сам Алгоритм
1.находим расстояния от точки u до k-ближайших-соседей образуя новый массив, где будет записан класс этой k-точки-соседа и расстояние
2.сортируем этот массив
3.считаем какие классы соседей встречаются чаще всего и даем предпочтение одному классу
Байесовские алгоритмы классификации
Байесовские алгоритмы классификации основаны на предположении, что  — вероятностное пространство с неизвестной плотностью распределения , из которого случайно и независимо извлекаются наблюдений. Вероятность  появления объектов класса  называется априорной вероятностью класса, плотности распределения  — функции правдоподобия классов.
Байесовский подход является классическим в теории распознавания образов и лежит в основе многих методов. Он опирается на теорему о том, что если плотности распределения классов известны, то алгоритм классификации, имеющий минимальную вероятность ошибок, можно выписать в явном виде.
Обозначим через  – величину потери алгоритмом  при неправильной классификации объекта класса .
Теорема.
Если известны априорные вероятности классов  и функции правдоподобия , то минимум среднего риска

достигается алгоритмом

Такой алгоритм называется оптимальным байесовским решающим правилом. Однако, на практике зачастую плотности распределения классов неизвестны и их приходится восстанавливать п о обучающей выборке. В этом случае байесовский алгоритм перестает быть оптимальным. Поэтому, чем лучше удастся восстановить функции правдоподобия, тем ближе будет к оптимальному построенный алгоритм. Существуют множество способов восстановления плотностей распределения по обучающей выборке, откуда как следствие большое количество разновидностей байесовских алгоритмов классификаций.

Линии уровня нормального распределения
Случайная величина  имеет нормальное (гауссовское) распределение с параметрами  и , если ее плотность задается выражением:

По центральной предельной теореме среднее арифметическое независимых случайных величин с ограниченными мат.ожиданием и дисперсией стремится к нормальному распределению.
Всего возможно 3 случая :
Если признаки некоррелируемы, т.е. коварициаонаая матрица - диагональна. И линии уровня имеют форму эллипсоидов с центром в  и осями параллельными осям координат.
Если признаки имеют одинаковые дисперсии , то линиями уровня являются сферами.
Если признаки коррелируемы, то ковариционная матрица не диагональна и линии уровня имеют форму эллипсоидов , оси которых повернуты вдоль собсвенных векторов ковариционной матрицы.
Линейные алгоритмы классификации
Линейная модель классификации
Пусть  и 
Если дискриминантная функция определяется как скалярное произведение вектора x и вектора параметров  , то получим линейный классификатор :

 задает гиперплоскость, разделяющую классы в пространстве .
Если вектор x находится по одну сторону гиперплоскости с ее направляющим вектором  , то объект x относится к классу +1, иначе к классу -1.
 - отступ обекта относительно алгоритма классификации.
 - монотонно невозрастающая функция отступа, мажорирующая пороговую функцию потерь: 
Метод стохастического градиента
Пусть задана обучающая выборка 
Требуется найти вектор параметров , при котором достигается минимум аппроксимированного эмпириского риска :

Т.о. метод стохастического градиента - то итерационные процесс на каждом шаге которого сдвигаемся в сторону противоположную вектору градиента ,до тех пор, пока вектор весов  не перестанет изменяться, причем вычисления градиента производится не на всех объектах обучения, а выбирается случайный объект (отсюда и название метода «стохастический»), на основе которого и происходят вычисления. В зависимости от функции потерь, которая используется в функционале эмпирического риска, будем получать различные линейные алгоритмы классификации.
Адаптивный линейный элемент
Возьмем в качестве функции потерь , тогда  , и получим правило обновения весов на каждой итерации метода стохастического градиента :

Персептрон Розенблата (Правило Хебба)
В качесстве функции потерь выберем :. Так же обновляетя правило обновления весов : .
Логистическая регрессия
Метод логистической регрессии основан на следующих вероятностных предположениях, которые имеют несколько интересных последствий:
-линейный классификатор оказывается оптимальным байесовским;
-однозначно определеяется фунция потерь;
-можно получать численные оценки вероятности принадлежности объеквто классам;
Существует следующее правило обновления весов для градиентного шага в методе стохастического градиента :
, где
сигмоид .
Функция потерь определяется следующим образом :

А так же вероятность принадлежности объекта x класса y :
.
